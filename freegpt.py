#   ‚ñà‚ñÄ‚ñÄ‚ÄÉ‚ñÑ‚ñÄ‚ñà‚ÄÉ ‚ÄÉ‚ñà‚ñÄ‚ñÑ‚ñÄ‚ñà‚ÄÉ‚ñà‚ñÄ‚ñà‚ÄÉ‚ñà‚ñÄ‚ñÑ‚ÄÉ‚ñà‚ñÄ
#   ‚ñà‚ñÄ‚ñë‚ÄÉ‚ñà‚ñÄ‚ñà‚ÄÉ ‚ÄÉ‚ñà‚ñë‚ñÄ‚ñë‚ñà‚ÄÉ‚ñà‚ñÑ‚ñà‚ÄÉ‚ñà‚ñÑ‚ñÄ‚ÄÉ‚ñÑ‚ñà

#   https://t.me/famods

# üîí    Licensed under the GNU AGPLv3
# üåê https://www.gnu.org/licenses/agpl-3.0.html

# ---------------------------------------------------------------------------------
# Name: FreeGPT
# Description: –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π ChatGPT. –ë–ï–ó API. –ë–ï–ó –ë–û–¢–û–í.
# meta developer: @FAmods
# meta banner: https://github.com/FajoX1/FAmods/blob/main/assets/banners/freegpt.png?raw=true
# requires: g4f[all]
# ---------------------------------------------------------------------------------

import asyncio
import logging

from g4f.client import Client

from .. import loader, utils

logger = logging.getLogger(__name__)

@loader.tds
class FreeGPT(loader.Module):
    """–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π ChatGPT. –ë–ï–ó API. –ë–ï–ó –ë–û–¢–û–í."""

    strings = {
        "name": "FreeGPT",

        "no_args": "<emoji document_id=5854929766146118183>‚ùå</emoji> <b>–ù—É–∂–Ω–æ </b><code>{}{} {}</code>",

        "asking_chatgpt": """<emoji document_id=5334675996714999970>üîÑ</emoji> <b>–°–ø—Ä–∞—à–∏–≤–∞—é ChatGPT...</b>

<i><emoji document_id=5370869711888194012>üëæ</emoji> –í—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç–µ –ø–æ–ª—É—á–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é stream_answer –≤ {prefix}cfg FreeGPT</i>""",
        "creating_image": "<emoji document_id=5334675996714999970>üîÑ</emoji> <b>–ì–µ–Ω–µ—Ä–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...</b>",

        "answer_text": """<emoji document_id=5818813162815753343>üë®‚Äçüíª</emoji> <b>–í–æ–ø—Ä–æ—Å:</b> {question}

<emoji document_id=5372981976804366741>ü§ñ</emoji> <b>–û—Ç–≤–µ—Ç:</b> {answer}

<emoji document_id=5424753383741346604>üñ•</emoji> <b>–ú–æ–¥–µ–ª—å</b>: <code>{model}</code>""",

        "photo_caption": """<emoji document_id=5375074927252621134>üñº</emoji> <b>–ü—Ä–æ–º–ø—Ç:</b> <code>{prompt}</code>
        
<emoji document_id=5424753383741346604>üñ•</emoji> <b>–ú–æ–¥–µ–ª—å</b>: <code>{model}</code>""",
    }

    def __init__(self):
        self.config = loader.ModuleConfig(
            loader.ConfigValue(
                "model",
                "gpt-4o",
                lambda: "–ú–æ–¥–µ–ª—å ChatGPT",
            ),
            loader.ConfigValue(
                "image_model",
                "flux",
                lambda: "–ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
            ),
            loader.ConfigValue(
                "role",
                "user",
                lambda: "–ö—Ç–æ —Ç—ã –¥–ª—è ChatGPT?",
            ),
            loader.ConfigValue(
                "stream_answer",
                False,
                lambda: "–û—Ç–≤–µ—Ç –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏",
                validator=loader.validators.Boolean()
            ),
            loader.ConfigValue(
                "stream_answer_delay",
                1.162131238129,
                lambda: "–ó–∞–¥–µ—Ä–∂–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏",
            ),
        )

    async def client_ready(self, client, db):
        self.db = db
        self._client = client

    @loader.command()
    async def gf(self, message):
        """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∫ ChatGPT"""
        q = utils.get_args_raw(message)
        if not q:
            return await utils.answer(message, self.strings["no_args"].format(self.get_prefix(), "gf", "[–≤–æ–ø—Ä–æ—Å]"))

        if not self.config['stream_answer']:
            await utils.answer(message, self.strings['asking_chatgpt'].format(prefix=self.get_prefix()))

            client = Client()
            response = client.chat.completions.create(
                model=self.config['model'],
                messages=[{"role": self.config['role'], "content": q}],
                stream=False,
            )

            return await utils.answer(message, self.strings['answer_text'].format(question=q, answer=response.choices[0].message.content.replace("Generated by BLACKBOX.AI, try unlimited chat https://www.blackbox.ai", "").strip(), model=self.config['model']))
        
        await utils.answer(message, self.strings['answer_text'].format(question=q, answer="<code>–ó–∞–≥—Ä—É–∑–∫–∞...</code>", model=self.config['model']))

        client = Client()
        response = client.chat.completions.create(
            model=self.config['model'],
            messages=[{"role": self.config['role'], "content": q}],
            stream=self.config['stream_answer'],
        )

        response_text = ""

        for chunk in response:
            if chunk.choices[0].delta.content:
                response_text += chunk.choices[0].delta.content
                await utils.answer(message, self.strings['answer_text'].format(question=q, answer=response_text+"...".replace("Generated by BLACKBOX.AI, try unlimited chat https://www.blackbox.ai", "").strip(), model=self.config['model']))
                await asyncio.sleep(self.config['stream_answer_delay'])

        return await utils.answer(message, self.strings['answer_text'].format(question=q, answer=response_text.replace("Generated by BLACKBOX.AI, try unlimited chat https://www.blackbox.ai", "").strip(), model=self.config['model']))    

    @loader.command()
    async def gfi(self, message):
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É"""
        prompt = utils.get_args_raw(message)
        if not prompt:
            return await utils.answer(message, self.strings["no_args"].format(self.get_prefix(), "gfi", "[–ø—Ä–æ–º–ø—Ç]"))

        m = await utils.answer(message, self.strings['creating_image'])

        client = Client()
        response = client.images.generate(
            model=self.config['image_model'],
            prompt=prompt.replace(" ", "+"),
        )

        await self.client.send_file(m.peer_id, response.data[0].url, force_document=True, caption=self.strings['photo_caption'].format(prompt=prompt, model=self.config['image_model']))
        await m.delete()